{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6pTwiBju2bi6",
        "FbSrWMro-43U",
        "P6Urh1M7_k9p",
        "a9K3HQaz_qu6"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This Notebook is intended to create the tfRecord file of the dataset so that It prevents the preprocessing every time and saves time and memory consumption."
      ],
      "metadata": {
        "id": "dmnYqQjF2esG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "6pTwiBju2bi6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGCVY03A2Fpy"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    This will create a ground truth file that I will use to\n",
        "    feed the ground truths to my model.\n",
        "\n",
        "    I will get the images and labels as a tf record file.\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import re\n",
        "import csv\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre processing the data"
      ],
      "metadata": {
        "id": "FbSrWMro-43U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to preprocess the image\n",
        "# You can normalize the images after they are being loaded from the tf record file.\n",
        "# image = image / 255.0  # Normalize image to have values between 0 and 1\n",
        "def preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Directly read as grayscale\n",
        "    new_size = (1000, 64)  # width, height\n",
        "    image = cv2.resize(image, new_size)\n",
        "    image = np.expand_dims(image, axis=-1)  # Add channel dimension\n",
        "    return image\n",
        "\n",
        "\n",
        "# Function to preprocess the text\n",
        "def preprocess_text(txt_pth, lt_pth):\n",
        "    english_chars = '[A-Za-z0-9۱۲۳۴۵۶۷۸۹۰]'\n",
        "    non_joiners = ['آ', 'ا', 'د', 'ڈ', 'ذ', 'ر', 'ڑ', 'ز', 'ژ', 'ں', 'و', 'ے', '\\\"', '،', '(', ')', '؟', '۔', '!', ':']\n",
        "    extra_char = ['\\\"', '،', '(', ')', '؟', '۔', '!', ':', 'ء']\n",
        "\n",
        "    with open(txt_pth, mode='r', encoding='utf-8-sig') as f:\n",
        "        try:\n",
        "            text = f.read()\n",
        "            ligatures = []\n",
        "            ligatures_return = []\n",
        "\n",
        "            words = text.split(' ')\n",
        "\n",
        "            for word in words:\n",
        "                ligature = ''\n",
        "                for char in word:\n",
        "                    if char not in non_joiners:\n",
        "                        ligature += char\n",
        "                    else:\n",
        "                        ligature += char\n",
        "                        ligatures.append(ligature)\n",
        "                        ligatures_return.append(ligature)\n",
        "                        ligature = ''\n",
        "                if ligature != '':\n",
        "                    ligatures.append(ligature)\n",
        "                    ligatures_return.append(ligature)\n",
        "\n",
        "            lig_list = []\n",
        "            for ligature in ligatures:\n",
        "                for char in ligature:\n",
        "                    result = re.findall(english_chars, char)\n",
        "                    if result:\n",
        "                        lig_list.append(char + '_isolated')\n",
        "                        ligature = ligature.replace(char, '')\n",
        "                    if char in extra_char:\n",
        "                        char_index = ligature.index(char)\n",
        "                        ligature = ligature.replace(char, '')\n",
        "                if ligature:\n",
        "                    if len(ligature) == 1:\n",
        "                        lig_list.append(ligature + '_isolated')\n",
        "                    else:\n",
        "                        lig_list.append(ligature[0] + '_initial')\n",
        "                        for middle in ligature[1:-1]:\n",
        "                            lig_list.append(middle + '_middle')\n",
        "                        lig_list.append(ligature[-1] + '_final')\n",
        "\n",
        "            # Load the label dictionary from the CSV file\n",
        "            with open(lt_pth, mode='r') as lt_file:\n",
        "                reader = csv.reader(lt_file)\n",
        "                label_dict = {row[0]: int(row[1]) for row in reader}\n",
        "\n",
        "            # Convert the ligatures to labels\n",
        "            labels = [label_dict.get(lig, 0) for lig in lig_list]\n",
        "\n",
        "            return labels\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"Exception occurred\")\n",
        "            print(e)\n",
        "            return []\n",
        "\n",
        "\n",
        "# Function to pad labels\n",
        "def pad_single_label(label, max_label_length, pad_value=999):\n",
        "    padded_label = pad_sequences([label], maxlen=max_label_length, padding='post', value=pad_value)\n",
        "    return padded_label[0]\n",
        "\n",
        "\n",
        "def pad_labels(labels, value=999):\n",
        "    max_label_length = max(len(label) for label in labels)\n",
        "    padded_labels = pad_sequences(labels, maxlen=max_label_length, padding='post', value=value)\n",
        "    return padded_labels\n"
      ],
      "metadata": {
        "id": "MTGLctaP-7pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Tf Record File"
      ],
      "metadata": {
        "id": "P6Urh1M7_k9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to parse TFRecord file\n",
        "def parse_tfrecord_fn(example):\n",
        "    feature_description = {\n",
        "        'image': tf.io.FixedLenFeature([], tf.string),\n",
        "        'label': tf.io.VarLenFeature(tf.int64),\n",
        "        'max_label_length': tf.io.FixedLenFeature([], tf.int64)\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, feature_description)\n",
        "    image = tf.io.decode_jpeg(example['image'], channels=1)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)  # Convert image to float32\n",
        "    label = tf.sparse.to_dense(example['label'])\n",
        "    max_label_length = example['max_label_length']\n",
        "    return image, label, max_label_length\n",
        "\n",
        "\n",
        "# Load TFRecord dataset\n",
        "def load_tfrecord_dataset(file_path):\n",
        "    dataset = tf.data.TFRecordDataset(file_path)\n",
        "    dataset = dataset.map(parse_tfrecord_fn)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Function to load the maximum sequence length\n",
        "def load_max_sequence_length(file_path):\n",
        "    with open(file_path, \"r\") as f:\n",
        "        line = f.readline().strip()\n",
        "        max_sequence_length = int(line.split(\": \")[1])\n",
        "    return max_sequence_length\n",
        "\n",
        "\n",
        "# Function to display images and print labels\n",
        "def display_image(image, label, max_label_length):\n",
        "    image_np = (image.numpy().squeeze() * 255).astype(np.uint8)  # Convert image back to uint8\n",
        "    label_np = label.numpy()\n",
        "\n",
        "    print(f\"Label: {label_np}\")\n",
        "    print(f\"Max Label Length: {max_label_length.numpy()}\")\n",
        "\n",
        "    cv2.imshow('Image', image_np)\n",
        "    if cv2.waitKey(0) & 0xFF == ord('q'):\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "# Define paths\n",
        "# tfrecord_file = '/home/cle-dl-05/Documents/3.PdfOCR/preprocessing_utils/dataset_30k.tfrecord'\n",
        "# max_label_length_file = '/home/cle-dl-05/Documents/3.PdfOCR/preprocessing_utils/max_label_length.txt'\n",
        "\n",
        "# # Load TFRecord dataset\n",
        "# dataset = load_tfrecord_dataset(tfrecord_file)\n",
        "\n",
        "# # Load maximum sequence length\n",
        "# max_sequence_length = load_max_sequence_length(max_label_length_file)\n",
        "# print(f\"Max Sequence Length: {max_sequence_length}\")\n",
        "\n",
        "# # Display some images from the first batch\n",
        "# for image, label, max_label_length in dataset.take(5):\n",
        "#     display_image(image, label, max_label_length)"
      ],
      "metadata": {
        "id": "d6M5LxAx_mun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Tf Record File"
      ],
      "metadata": {
        "id": "a9K3HQaz_qu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Function to parse TFRecord file\n",
        "def parse_tfrecord_fn(example):\n",
        "    feature_description = {\n",
        "        'image': tf.io.FixedLenFeature([], tf.string),\n",
        "        'label': tf.io.VarLenFeature(tf.int64),\n",
        "        'max_label_length': tf.io.FixedLenFeature([], tf.int64)\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, feature_description)\n",
        "    image = tf.io.decode_jpeg(example['image'], channels=1)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)  # Convert image to float32\n",
        "    label = tf.sparse.to_dense(example['label'])\n",
        "    max_label_length = example['max_label_length']\n",
        "    return image, label, max_label_length\n",
        "\n",
        "# Load TFRecord file\n",
        "def load_tfrecord_dataset(file_path):\n",
        "    dataset = tf.data.TFRecordDataset(file_path)\n",
        "    dataset = dataset.map(parse_tfrecord_fn)\n",
        "    return dataset\n",
        "\n",
        "# Define paths\n",
        "tfrecord_file = '/home/cle-dl-05/Documents/3.PdfOCR/preprocessing_utils/30k_dataset.tfrecord'\n",
        "\n",
        "# Load TFRecord dataset\n",
        "dataset = load_tfrecord_dataset(tfrecord_file)\n",
        "\n",
        "# Split dataset into train and test\n",
        "train_size = int(0.8 * tf.data.experimental.cardinality(dataset).numpy())  # Convert to numpy array\n",
        "test_size = tf.data.experimental.cardinality(dataset).numpy() - train_size\n",
        "train_dataset = dataset.take(train_size)\n",
        "test_dataset = dataset.skip(train_size)\n",
        "\n",
        "# Print sizes of train and test datasets\n",
        "print(\"Train Dataset Size:\", train_size)\n",
        "print(\"Test Dataset Size:\", test_size)\n",
        "\n",
        "# Print a batch from the training data\n",
        "for image, label, max_label_length in train_dataset.take(1):\n",
        "    print(\"Image Shape:\", image.shape)\n",
        "    print(\"Label:\", label)\n",
        "    print(\"Max Label Length:\", max_label_length)\n"
      ],
      "metadata": {
        "id": "MBe1Ove4_ugY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}